{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe13419-cca3-4746-a62c-65e0f23640b8",
   "metadata": {},
   "source": [
    "`Q1. Install the Package`\\\n",
    "To get started with Weights & Biases you'll need to install the appropriate Python package.\n",
    "\n",
    "For this we recommend creating a separate Python environment, for example, you can use conda environments, and then install the package there with pip or conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64857804-d091-42d6-bb4b-03b246a5e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e72ca-3a34-4f13-be29-c69550ee2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f56710d-de8f-4bda-83ab-35848a5731f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login() # then follow the link to get your API key and paste in box below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6ab86-afb8-4fac-8f81-36ab67ca88c2",
   "metadata": {},
   "source": [
    "Once you installed the package, run the command `wandb --version` and check the output.\n",
    "\n",
    "What's the version that you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776a078b-15de-4ffa-8323-16c8402e692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb, version 0.13.5\n"
     ]
    }
   ],
   "source": [
    "!wandb --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b4050-71ae-4476-aac6-83df18802ce0",
   "metadata": {},
   "source": [
    "`Q2. Download and preprocess the data`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060aec45-e366-4de3-a9c1-c1c16f1af5c6",
   "metadata": {},
   "source": [
    "We'll use the Green Taxi Trip Records dataset to predict the amount of tips for each trip.\n",
    "\n",
    "Download the data for January, February and March 2022 in parquet format from here.\n",
    "\n",
    "Use the script `preprocess_data.py` located in the folder homework-wandb to preprocess the data.\n",
    "\n",
    "The script will:\n",
    "\n",
    "- initialize a Weights & Biases run.\n",
    "- load the data from the folder <TAXI_DATA_FOLDER> (the folder where you have downloaded the data)\n",
    "- fit a DictVectorizer on the training set (January 2022 data)\n",
    "- save the preprocessed datasets and the DictVectorizer to your Weights & Biases dashboard as an artifact of type `preprocessed_dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84068d74-529b-43bd-90ba-d05ddaeed0fa",
   "metadata": {},
   "source": [
    "`preprocess_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c8d4b-81dc-4983-8151-7e06e5e73973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import click\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "def dump_pickle(obj, filename: str):\n",
    "    with open(filename, \"wb\") as f_out:\n",
    "        return pickle.dump(obj, f_out)\n",
    "\n",
    "\n",
    "def read_dataframe(filename: str):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df[\"duration\"] = df[\"lpep_dropoff_datetime\"] - df[\"lpep_pickup_datetime\"]\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess(df: pd.DataFrame, dv: DictVectorizer, fit_dv: bool = False):\n",
    "    df[\"PU_DO\"] = df[\"PULocationID\"] + \"_\" + df[\"DOLocationID\"]\n",
    "    categorical = [\"PU_DO\"]\n",
    "    numerical = [\"trip_distance\"]\n",
    "    dicts = df[categorical + numerical].to_dict(orient=\"records\")\n",
    "    if fit_dv:\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "    return X, dv\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--wandb_project\", help=\"Name of Weights & Biases project\")\n",
    "@click.option(\"--wandb_entity\", help=\"Name of Weights & Biases entity\")\n",
    "@click.option(\n",
    "    \"--raw_data_path\", help=\"Location where the raw NYC taxi trip data was saved\"\n",
    ")\n",
    "@click.option(\"--dest_path\", help=\"Location where the resulting files will be saved\")\n",
    "def run_data_prep(\n",
    "    wandb_project: str,\n",
    "    wandb_entity: str,\n",
    "    raw_data_path: str,\n",
    "    dest_path: str,\n",
    "    dataset: str = \"green\",\n",
    "):\n",
    "    # Initialize a Weights & Biases run\n",
    "    wandb.init(project=wandb_project, entity=wandb_entity, job_type=\"preprocess\")\n",
    "\n",
    "    # Load parquet files\n",
    "    df_train = read_dataframe(\n",
    "        os.path.join(raw_data_path, f\"{dataset}_tripdata_2022-01.parquet\")\n",
    "    )\n",
    "    df_val = read_dataframe(\n",
    "        os.path.join(raw_data_path, f\"{dataset}_tripdata_2022-02.parquet\")\n",
    "    )\n",
    "    df_test = read_dataframe(\n",
    "        os.path.join(raw_data_path, f\"{dataset}_tripdata_2022-03.parquet\")\n",
    "    )\n",
    "\n",
    "    # Extract the target\n",
    "    target = \"tip_amount\"\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_val[target].values\n",
    "    y_test = df_test[target].values\n",
    "\n",
    "    # Fit the DictVectorizer and preprocess data\n",
    "    dv = DictVectorizer()\n",
    "    X_train, dv = preprocess(df_train, dv, fit_dv=True)\n",
    "    X_val, _ = preprocess(df_val, dv, fit_dv=False)\n",
    "    X_test, _ = preprocess(df_test, dv, fit_dv=False)\n",
    "\n",
    "    # Create dest_path folder unless it already exists\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "    # Save DictVectorizer and datasets\n",
    "    dump_pickle(dv, os.path.join(dest_path, \"dv.pkl\"))\n",
    "    dump_pickle((X_train, y_train), os.path.join(dest_path, \"train.pkl\"))\n",
    "    dump_pickle((X_val, y_val), os.path.join(dest_path, \"val.pkl\"))\n",
    "    dump_pickle((X_test, y_test), os.path.join(dest_path, \"test.pkl\"))\n",
    "\n",
    "    artifact = wandb.Artifact(\"NYC-Taxi\", type=\"preprocessed_dataset\")\n",
    "    artifact.add_dir(dest_path)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_data_prep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d44ff-51b2-44ae-8fbf-5af1ac1879e1",
   "metadata": {},
   "source": [
    "Your task is to download the datasets and then execute this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6b254-e6cd-4f22-85fd-3f12d45b2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "python preprocess_data.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --raw_data_path <TAXI_DATA_FOLDER> \\\n",
    "  --dest_path ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e8819-ade4-4266-a8eb-b5767e9129ca",
   "metadata": {},
   "source": [
    "Tip: go to `02-experiment-tracking/homework-wandb/` folder before executing the command and change the value of `<WANDB_PROJECT_NAME>` to the name of your Weights & Biases project, `<WANDB_USERNAME>` to your Weights & Biases username, and `<TAXI_DATA_FOLDER>` to the location where you saved the data.\n",
    "\n",
    "Once you navigate to the Files tab of your artifact on your Weights & Biases page, what's the size of the saved DictVectorizer file?\n",
    "\n",
    "- 54 kB\n",
    "- 154 kB\n",
    "- 54 MB\n",
    "- 154 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9f92e7-6ea1-457b-86e3-7e8bf3e70b21",
   "metadata": {},
   "source": [
    "`Q3. Train a model with Weights & Biases logging`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae8753-6da5-4729-899b-f233816172de",
   "metadata": {},
   "source": [
    "We will train a RandomForestRegressor (from Scikit-Learn) on the taxi dataset.\n",
    "\n",
    "We have prepared the training script `train.py` for this exercise, which can be also found in the folder `homework-wandb`.\n",
    "\n",
    "The script will:\n",
    "\n",
    "- initialize a Weights & Biases run.\n",
    "- load the preprocessed datasets by fetching them from the Weights & Biases artifact previously created,\n",
    "- train the model on the training set,\n",
    "- calculate the MSE score on the validation set and log it to Weights & Biases,\n",
    "- save the trained model and log it to Weights & Biases as a model artifact.\n",
    "\n",
    "Your task is to **modify* the script to enable to add Weights & Biases logging, execute the script and then check the Weights & Biases run UI to check that the experiment run was properly tracked.\n",
    "\n",
    "`TODO 1:` log mse to Weights & Biases under the key \"MSE\"\n",
    "\n",
    "`TODO 2:` log regressor.pkl as an artifact of type model, refer to the [official docs](https://docs.wandb.ai/guides/artifacts) in order to know more about logging artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45674de5-cf65-46ac-bd1f-4657e481ae56",
   "metadata": {},
   "source": [
    "Modify the following script :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb8b24-9783-4df9-81d8-213b560cc388",
   "metadata": {},
   "source": [
    "`train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba2721-b699-496b-84af-3d623f882073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import click\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def load_pickle(filename: str):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--wandb_project\", help=\"Name of Weights & Biases project\")\n",
    "@click.option(\"--wandb_entity\", help=\"Name of Weights & Biases entity\")\n",
    "@click.option(\n",
    "    \"--data_artifact\",\n",
    "    help=\"Address of the Weights & Biases artifact holding the preprocessed data\",\n",
    ")\n",
    "@click.option(\"--random_state\", default=0, help=\"Random state\")\n",
    "@click.option(\"--max_depth\", default=10, help=\"Max tree depth\")\n",
    "def run_train(\n",
    "    wandb_project: str,\n",
    "    wandb_entity: str,\n",
    "    data_artifact: str,\n",
    "    max_depth: int,\n",
    "    random_state: int,\n",
    "):\n",
    "    # Initialize a Weights & Biases run\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        entity=wandb_entity,\n",
    "        job_type=\"train\",\n",
    "        config={\"max_depth\": max_depth, \"random_state\": random_state},\n",
    "    )\n",
    "\n",
    "    # Fetch the preprocessed dataset from artifacts\n",
    "    artifact = wandb.use_artifact(data_artifact, type=\"preprocessed_dataset\")\n",
    "    data_path = artifact.download()\n",
    "\n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "    # Define the XGBoost Regressor Mode, train the model and perform prediction\n",
    "    rf = RandomForestRegressor(max_depth=max_depth, random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_val)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    # TODO: Log `mse` to Weights & Biases under the key `\"MSE\"`\n",
    "\n",
    "    with open(\"regressor.pkl\", \"wb\") as f:\n",
    "        pickle.dump(rf, f)\n",
    "\n",
    "    # TODO: Log `regressor.pkl` as an artifact of type `model`\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897d505-c1f1-40a5-8693-97eb47444e59",
   "metadata": {},
   "source": [
    "You can then run the **modified** script using :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5af0f-b963-471c-8067-3e5a299e483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --data_artifact \"<WANDB_PROJECT_NAME>/<WANDB_USERNAME>/NYC-Taxi:v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf29bc3-eb2f-4e0d-a2da-02085e27900a",
   "metadata": {},
   "source": [
    "- `Tip 1:` You can find the artifact address under the Usage tab in the respective artifact's page.\n",
    "\n",
    "- `Tip 2:` don't modify the hyperparameters of the model to make sure that the training will finish quickly.\n",
    "\n",
    "Once you have successfully ran the script, navigate the Overview section of the run in the Weights & Biases UI and scroll down to the Configs. \n",
    "\n",
    "What is the value of the max_depth parameter:\n",
    "\n",
    "- 4\n",
    "- 6\n",
    "- 8\n",
    "- 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5b611-e97d-4d44-a19c-2dd271bc688e",
   "metadata": {},
   "source": [
    "`Q4. Tune model hyperparameters`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca33a82-a430-49fd-9648-6e9fc7faedae",
   "metadata": {},
   "source": [
    "Now let's try to reduce the validation error by tuning the hyperparameters of the RandomForestRegressor using [Weights & Biases Sweeps](https://docs.wandb.ai/guides/sweeps). We have prepared the script `sweep.py` for this exercise in the homework-wandb directory.\n",
    "\n",
    "Your task is to **modify** `sweep.py` to pass the parameters `n_estimators`, `min_samples_split` and `min_samples_leaf` from config to RandomForestRegressor inside the `run_train()` function. Then we will run the sweep to figure out not only the best best of hyperparameters for training our model, but also to analyze the most optimum trends in different hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb8c9c-0435-449a-b886-c5a912bbedf5",
   "metadata": {},
   "source": [
    "`sweep.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f07ce-aee9-4151-969e-9f349d7f585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import click\n",
    "from functools import partial\n",
    "\n",
    "import wandb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def load_pickle(filename: str):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "\n",
    "def run_train(data_artifact: str):\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Fetch the preprocessed dataset from artifacts\n",
    "    artifact = wandb.use_artifact(data_artifact, type=\"preprocessed_dataset\")\n",
    "    data_path = artifact.download()\n",
    "\n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "    # Define the XGBoost Regressor Mode, train the model and perform prediction\n",
    "    # TODO: Pass the parameters n_estimators, min_samples_split, min_samples_leaf from `config` to `RandomForestRegressor`\n",
    "    rf = RandomForestRegressor(max_depth=config.max_depth, random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_val)\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    wandb.log({\"MSE\": mse})\n",
    "\n",
    "    with open(\"regressor.pkl\", \"wb\") as f:\n",
    "        pickle.dump(rf, f)\n",
    "\n",
    "    artifact = wandb.Artifact(f\"{wandb.run.id}-model\", type=\"model\")\n",
    "    artifact.add_file(\"regressor.pkl\")\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "\n",
    "SWEEP_CONFIG = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"MSE\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"max_depth\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 1,\n",
    "            \"max\": 20,\n",
    "        },\n",
    "        \"n_estimators\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 10,\n",
    "            \"max\": 50,\n",
    "        },\n",
    "        \"min_samples_split\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 2,\n",
    "            \"max\": 10,\n",
    "        },\n",
    "        \"min_samples_leaf\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 1,\n",
    "            \"max\": 4,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--wandb_project\", help=\"Name of Weights & Biases project\")\n",
    "@click.option(\"--wandb_entity\", help=\"Name of Weights & Biases entity\")\n",
    "@click.option(\n",
    "    \"--data_artifact\",\n",
    "    help=\"Address of the Weights & Biases artifact holding the preprocessed data\",\n",
    ")\n",
    "@click.option(\"--count\", default=5, help=\"Number of iterations in the sweep\")\n",
    "def run_sweep(wandb_project: str, wandb_entity: str, data_artifact: str, count: int):\n",
    "    sweep_id = wandb.sweep(SWEEP_CONFIG, project=wandb_project, entity=wandb_entity)\n",
    "    wandb.agent(sweep_id, partial(run_train, data_artifact=data_artifact), count=count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_sweep()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1250897-a541-4a55-8b6e-82f1c933dc39",
   "metadata": {},
   "source": [
    "We can then run the **modified** sweep using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bad0d-912a-4f8b-9300-f6d77e1ad9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "python sweep.py \\\n",
    "  --wandb_project <WANDB_PROJECT_NAME> \\\n",
    "  --wandb_entity <WANDB_USERNAME> \\\n",
    "  --data_artifact \"<WANDB_PROJECT_NAME>/<WANDB_USERNAME>/NYC-Taxi:v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9344c-b003-41b0-a6f9-a408bb5c1d2a",
   "metadata": {},
   "source": [
    "This command will run the sweep for 5 iterations using the Bayesian Optimization and HyperBand method proposed by the paper [BOHB: Robust and Efficient Hyperparameter Optimization at Scale](https://arxiv.org/abs/1807.01774). You can take a look at the sweep on your Weights & Biases dashboard, take a look at the Parameter Inportance Panel and the Parallel Coordinates Plot to determine, and analyze which hyperparameter is the most important:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acf484-149d-43d4-b376-8a1115642ac3",
   "metadata": {},
   "source": [
    "`Q5. Link the best model to the model registry`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e4d85-8043-49c2-9faf-868fb688cf05",
   "metadata": {},
   "source": [
    "Now that we have obtained the optimal set of hyperparameters and trained the best model, we can assume that we are ready to test some of these models in production. In this exercise, you'll create a model registry and link the best model from the Sweep to the model registry.\n",
    "\n",
    "First, you will need to create a Registered Model to hold all the candidate models for your particular modeling task. You can refer to [this section](https://docs.wandb.ai/guides/models/walkthrough#1-create-a-new-registered-model) of the official docs to learn how to create a registered model using the Weights & Biases UI.\n",
    "\n",
    "Once you have created the Registered Model successfully, you can navigate to the best run of your sweep, navigate to the model artifact created by the particular run, and click on the Link to Registry option from the UI. This would link the model artifact to the Registered Model. You can choose to add some suitable aliases for the Registered Model, such as production, best, etc.\n",
    "\n",
    "Now that the model artifact is linked to the Registered Model, which of these information do we see on the Registered Model UI?\n",
    "\n",
    "- Versioning\n",
    "- Metadata\n",
    "- Aliases\n",
    "- Metric (MSE)\n",
    "- Source run\n",
    "- All of these\n",
    "- None of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c33d88-231f-4b94-9e86-8fdfe378c4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
